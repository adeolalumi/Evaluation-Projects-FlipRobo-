{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8126325e-7284-4d38-8823-1f3281a1d8d8",
   "metadata": {},
   "source": [
    "Engineer: Adeola Odunewu\n",
    "Intern: FlipRobo LLC DS1123\n",
    "Project: Zomato Resturant Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df893ff-2cb1-4dba-9c48-9d962c9aa68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*deprecated.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File paths for the CSV files\n",
    "url1 = 'https://github.com/FlipRoboTechnologies/ML_-Datasets/raw/main/Z_Restaurant/Country-Code.xlsx'\n",
    "url2 = 'https://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Z_Restaurant/zomato.csv'\n",
    "\n",
    "# Read CSV files into DataFrame objects\n",
    "df1 = pd.read_excel(url1, engine='openpyxl')  # Specify the engine as 'openpyxl' for reading Excel files\n",
    "df2 = pd.read_csv(url2, encoding='latin1')    # Specify the encoding as 'latin1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc50c5d-5ca4-4125-ae49-d2813ad92ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the DataFrames\n",
    "merged_df = pd.merge(df1, df2, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c0b96",
   "metadata": {},
   "source": [
    "The two file are merged together to form a single DataFrame called 'merged_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b487176-4dc5-46af-85e7-a7182984d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d80ce-96b2-4592-8279-e8bb73b265b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Data Type\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc2fe6-91ad-43a4-8a5c-5ce868c3ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values\n",
    "missing_data = merged_df.isnull().sum()\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc013a-3852-485c-9e0d-92e872815a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode of the 'Cuisines' column\n",
    "cuisine_mode = merged_df['Cuisines'].mode().iloc[0]\n",
    "\n",
    "# Fill missing values in the 'Cuisines' column with the mode\n",
    "merged_df['Cuisines'].fillna(cuisine_mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e72a7-bb1e-40d0-b0c2-7ac2a4abb425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical columns to object type for compatibility\n",
    "for col in merged_df.columns:\n",
    "    if merged_df[col].dtype == 'category':\n",
    "        merged_df[col] = merged_df[col].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24833e38-b684-4f48-b5ee-053ad5b976fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting and describing statistics for categorical columns (both object and category types) in the DataFrame\n",
    "cate_df = merged_df.select_dtypes(include=['object', 'category'])\n",
    "cate_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a373a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05979178-3d69-4f69-86e2-0fdd6999c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns with numeric dtypes (integers and floats)\n",
    "numeric_df = merged_df.select_dtypes(include=['int64', 'float64'])\n",
    "numeric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ca59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting descritive graph\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(numeric_df.describe(), annot=True, fmt=\".2f\", linewidths=0.2, linecolor='black',cmap='coolwarm')\n",
    "plt.title(\"Numerical Data Descriptive Statistics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742aade-f099-47d5-b165-01e68a34f03a",
   "metadata": {},
   "source": [
    "The analysis optimizes our DataFrame by converting object type columns to category type to reduce memory usage, and category type columns to object type for compatibility with certain operations. Extracting and describing statistics for categorical columns helps us understand the distribution and frequency of categorical data. Similarly, generating summary statistics for numeric columns provides insights into their central tendencies and variability. \n",
    "\n",
    "These steps ensure efficient memory usage and compatibility, enhancing the performance and accuracy of our data analysis. Ultimately, this process is\n",
    "crucial for maintaining a manageable and insightful dataset, facilitating better decision-making and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedc016-da0d-4564-8f10-6081785c8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a histogram plot of the Average Cost for two column from the DataFrame merged_df\n",
    "sns.histplot(data = merged_df, x = 'Aggregate rating')\n",
    "plt.title('Aggregate Rating (Fig 1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc94f34-e983-48ab-a387-4f057943d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the figure\n",
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "# Create a count plot of the 'Country' column from the DataFrame merged_df\n",
    "sns.countplot(data=merged_df, x='Country')\n",
    "\n",
    "# Set plot title and axis labels\n",
    "plt.title('Distribution of Restaurants by Country')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels for better readability if necessary\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b659d4-8e20-4778-ab06-59640c649957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Create a histogram plot of the Price range for two column from the DataFrame merged_df\n",
    "sns.histplot(data = merged_df, x = 'Price range')\n",
    "plt.title('Price range (Fig 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54503b86-2bf3-48f3-894a-fdd3136ceebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(78, 45))\n",
    "\n",
    "# Create a treemap plot with Price range as the sizes and Cuisines as the labels from the DataFrame merged_df\n",
    "squarify.plot(sizes=merged_df['Price range'], label=merged_df['Cuisines'], alpha=.8, pad=True)\n",
    "\n",
    "\n",
    "\n",
    "# Add title and remove axes\n",
    "plt.title('Cuisine Popularity Treemap', fontsize=20)\n",
    "plt.axis('off')\n",
    "\n",
    "# Create a legend\n",
    "# Generate a list of unique labels and their corresponding colors\n",
    "cuisines = merged_df['Cuisines'].unique()\n",
    "handles = [plt.Rectangle((0, 0), 1, 1, color=plt.cm.viridis(i / float(len(cuisines)))) for i in range(len(cuisines))]\n",
    "\n",
    "# Add the legend\n",
    "plt.legend(handles, cuisines, title=\"Cuisines\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "'''import squarify\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(68,45))\n",
    "# Create a treemap plot of the Price range for two column from the DataFrame merged_df\n",
    "\n",
    "squarify.plot(sizes=merged_df['Price range'], label=merged_df['Cuisines'], alpha=.8)\n",
    "plt.title('Cuisine Popularity Treemap')\n",
    "plt.axis('off')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be97a9b-5c2d-4bb4-834d-d4fb7bca7ea9",
   "metadata": {},
   "source": [
    "Please note that the plot above depicts a treemap of approximately 1825 varieties of cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5415ec-78d3-443b-aa40-ea397cbc26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the figure\n",
    "plt.figure(figsize=(45, 15))\n",
    "\n",
    "# Create a count plot of the Price range for different Cuisines\n",
    "sns.countplot(data=merged_df, x='Cuisines')\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('Count of Different Cuisines', fontsize=16)\n",
    "plt.xlabel('Cuisines', fontsize=6)\n",
    "plt.ylabel('Count', fontsize= 6)\n",
    "\n",
    "# Rotate x-ticks for better readability if there are many cuisines\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093b659-bbfb-4bd5-a65a-db86b87941b4",
   "metadata": {},
   "source": [
    "The analysis creates histograms to visualize the Aggregate rating and Price range distribution, helping us understand their spread and central tendencies. \n",
    "\n",
    "A treemap illustrates the relative popularity of different cuisines by plotting the Price range against Cuisines, revealing which cuisines are most \n",
    "prevalent. A count plot shows the frequency of different cuisines, aiding in identifying the most common ones. These steps are crucial for effective data visualization and optimization, providing clearer insights and more efficient data handling for better decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec885f-f811-4674-945f-2a36f4d6b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis for numeric_df is the DataFrame\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the correlation matrix including the target variable\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483551c-e2a4-4922-993f-d5dd80a75689",
   "metadata": {},
   "source": [
    "The correlation coefficients offer valuable insights into the dataset's dynamics. With a coefficient of 0.81 between average price and vote, a strong \n",
    "positive relationship emerges, indicating that higher restaurant prices often correlate with higher votes, potentially reflecting superior quality or \n",
    "satisfaction. Similarly, a coefficient of 0.51 between vote and price range suggests a moderate positive connection, implying that establishments with \n",
    "broader price ranges tend to garner more votes. Additionally, the correlation of 0.47 between average cost for two and price range underscores how \n",
    "pricing influences customer engagement, suggesting that venues with wider price ranges attract a more diverse clientele.\n",
    "\n",
    "On the other hand, the negative correlation of -0.33 between aggregate rating and restaurant ID suggests that as the identification number rises, \n",
    "there's a tendency for aggregate ratings to decrease slightly, hinting at variations in rating patterns across different restaurant IDs. Essentially, \n",
    "this implies that newer establishments, indicated by higher ID numbers, may receive marginally lower aggregate ratings compared to their older \n",
    "counterparts. These findings highlight the substantial impact of pricing and identification variables on customer engagement and perception within \n",
    "the dataset, underscoring their pivotal role in subsequent analyses and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fd531-a6ea-4ae5-8d44-e37d6a8b2a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identifying Outliers\n",
    "\n",
    "# Calculate z-scores for each data point\n",
    "z_scores = ((numeric_df - numeric_df.mean()) / numeric_df.std()).abs()\n",
    "\n",
    "# Define threshold for outliers (if z-score greater than 3)\n",
    "outlier_threshold = 3\n",
    "\n",
    "# Identify outliers using the z-score method\n",
    "outlier_conditions = (z_scores > outlier_threshold)\n",
    "\n",
    "# Display rows containing outliers\n",
    "outliers = merged_df[outlier_conditions.any(axis=1)]\n",
    "print(\"Rows with outliers:\")\n",
    "print(outliers)\n",
    "\n",
    "# Visualize the outliers using a boxplot\n",
    "plt.figure(figsize=(9, 7))  \n",
    "sns.boxplot(data= numeric_df) \n",
    "plt.title(\"Boxplot with Outliers\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797240b1-72c9-471d-ae5b-2f5cbde8260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers\n",
    "# Step 1: Identify Variables with Outliers\n",
    "outlier_vars = ['Average Cost for two', 'Price range', 'Aggregate rating','Votes']\n",
    "\n",
    "# Step 2: Apply Logarithmic Transformation\n",
    "# Logarithmic Transformation\n",
    "log_transformed_vars = merged_df[outlier_vars].apply(lambda x: np.log(x + 1))  # Adding 1 to avoid log(0) issues\n",
    "\n",
    "# Replace the original columns with the transformed columns\n",
    "merged_df[outlier_vars] = log_transformed_vars  \n",
    "\n",
    "# Visualize the transformed data\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, col in enumerate(outlier_vars):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(log_transformed_vars[col], bins=30, alpha=0.5, color='blue', label='Log Transformation')\n",
    "    plt.legend()\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be43df-66cc-437f-a87d-83192db19f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing value\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183b826-c8ce-496b-b528-11dbdaff3440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Transform the data into the principal components\n",
    "pca_data = pca.transform(scaled_data)\n",
    "\n",
    "# Extracting explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# df_columns is a list of column names in the df1 DataFrame\n",
    "df_columns = numeric_df.columns.tolist()\n",
    "\n",
    "# Create a DataFrame to store the variable loadings for each principal component\n",
    "loadings_df = pd.DataFrame(pca.components_, columns=df_columns)\n",
    "\n",
    "# Print the variable loadings for each principal component\n",
    "for i in range(loadings_df.shape[0]):\n",
    "    print(f\"Principal Component {i+1} Loadings:\")\n",
    "    print(loadings_df.iloc[i].sort_values(ascending=False))\n",
    "    print()\n",
    "# Create a DataFrame to examine the principal components\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=[f\"PC{i+1}\" for i in range(pca_data.shape[1])])\n",
    "\n",
    "# Visualize the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ca2e4-2822-428b-86e6-a351fbe347b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to examine the principal components\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=[f\"PC{i+1}\" for i in range(pca_data.shape[1])])\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of the first two principal components\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)\n",
    "plt.title('PCA - PC1 vs PC2')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57c607",
   "metadata": {},
   "source": [
    "The elongated clusters observed in PCA1 and PCA2 indicate a clear directional trend in the data. In contrast, the outlier\n",
    "points could be indicative of data points with unique characteristics that differentiate them from the main clusters or could\n",
    "potentially be errors in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f0553-8dda-4f1a-b4c7-77fff5169714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create a DataFrame to store VIF values\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = numeric_df.columns\n",
    "\n",
    "# Calculate VIF for each predictor variable\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(numeric_df.values, i) for i in range(len(numeric_df.columns))]\n",
    "\n",
    "# Print the VIF DataFrame\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c341e8-e8f3-46fb-83a2-6f51a37a31cf",
   "metadata": {},
   "source": [
    "The variance inflation factors (VIFs) for the predictor variables are all less than 10, indicating that the data do not exhibit extreme multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607e35d-62a5-44c9-9cfd-13fe73038ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features\n",
    "features = ['Country Code', 'Restaurant ID', 'Longitude', 'Latitude', 'Price range', 'Aggregate rating', 'Votes']\n",
    "categorical_features = ['Country', 'Restaurant Name', 'City', 'Address', 'Locality', 'Locality Verbose', 'Cuisines', 'Currency', 'Has Table booking', 'Has Online delivery', 'Is delivering now', 'Switch to order menu', 'Rating color', 'Rating text']\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "X_numerical = merged_df[features]\n",
    "X_categorical = pd.get_dummies(merged_df[categorical_features])  # One-hot encode categorical columns\n",
    "X = pd.concat([X_numerical, X_categorical], axis=1)  # Combine numerical and encoded categorical features\n",
    "y = merged_df['Average Cost for two']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Fit and transform the scaled features\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Print the coefficients\n",
    "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': lr_model.coef_})\n",
    "coefficients_sorted = coefficients.reindex(coefficients['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "print(coefficients_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a8fcb-a101-4452-a9ce-dfcdb9dff19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_model.coef_, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4df2ae-42be-4e6c-a8c8-89ff584fa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "Xi = merged_df[['Country Code', 'Longitude', 'Latitude', 'Price range', 'Aggregate rating', 'Votes']]  # Features (excluding categorical columns)\n",
    "X_categoricali = pd.get_dummies(merged_df[['Has Table booking', 'Rating color', 'Rating text', 'Cuisines', 'Currency', 'Locality', 'Locality Verbose']])  # One-hot encode categorical columns\n",
    "Xi = pd.concat([Xi, X_categoricali], axis=1)  # Combine numeric al and encoded categorical features\n",
    "yi = merged_df['Average Cost for two']\n",
    "\n",
    "# Standardize features \n",
    "scaler = StandardScaler()\n",
    "X_scaledi = scaler.fit_transform(Xi)  # Fit and transform the scaled features\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_traini, X_testi, y_traini, y_testi = train_test_split(X_scaledi, yi, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e97d6-dbb5-416d-93f2-b307147aada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define regression models\n",
    "regressors = {\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Evaluate and print results for each regressor\n",
    "for name, reg in regressors.items():\n",
    "    # Perform 5-fold cross-validation for MSE\n",
    "    scores_mse = cross_val_score(reg, Xi, yi, cv=5, scoring='neg_mean_squared_error')\n",
    "    # Perform 5-fold cross-validation for R²\n",
    "    scores_r2 = cross_val_score(reg, Xi, yi, cv=5, scoring='r2')\n",
    "    \n",
    "    # Compute mean and standard deviation of each metric\n",
    "    mean_mse, std_mse = -scores_mse.mean(), scores_mse.std()\n",
    "    mean_r2, std_r2 = scores_r2.mean(), scores_r2.std()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Regressor: {name}\")\n",
    "    print(f\"Mean Squared Error: {mean_mse:.4f} +/- {std_mse:.4f}\")\n",
    "    print(f\"R-squared (R²) Score: {mean_r2:.4f} +/- {std_r2:.4f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b299956",
   "metadata": {},
   "source": [
    "RandomForest has the lowest MSE (0.9298), followed by GradientBoosting (0.9635), SVR (1.0231), and ElasticNet (1.2164). In terms of R-squared scores, RandomForest again leads with 0.5378, followed by GradientBoosting at 0.5151, SVR at 0.2543, and ElasticNet at -0.0304. A negative R-squared score, as seen with ElasticNet, indicates that the model performs worse than simply predicting the average of the dependent variable. Given these results, hyperparameter tuning will be conducted using RandomForest to further improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf50b3-c98c-4a65-a906-eb78906fda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define the Random Forest regressor\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "# Evaluate the Random Forest regressor using 5-fold cross-validation for MSE and R²\n",
    "scores_mse = cross_val_score(random_forest, Xi, yi, cv=5, scoring='neg_mean_squared_error')\n",
    "scores_r2 = cross_val_score(random_forest, Xi, yi, cv=5, scoring='r2')\n",
    "\n",
    "# Compute mean and standard deviation of each metric\n",
    "mean_mse, std_mse = -scores_mse.mean(), scores_mse.std()\n",
    "mean_r2, std_r2 = scores_r2.mean(), scores_r2.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Regressor: Random Forest\")\n",
    "print(f\"Mean Squared Error: {mean_mse:.4f} +/- {std_mse:.4f}\")\n",
    "print(f\"R-squared (R²) Score: {mean_r2:.4f} +/- {std_r2:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train the Random Forest regressor on the full training set\n",
    "random_forest.fit(X_traini, y_traini)\n",
    "\n",
    "# Save the trained model using joblib\n",
    "joblib_filename = \"RandomForest_model.joblib\"\n",
    "joblib.dump(random_forest, joblib_filename)\n",
    "print(f\"Saved Random Forest model to {joblib_filename}\")\n",
    "\n",
    "print('Hypertuning')\n",
    "\n",
    "'''\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Initialize the RandomForestRegressor with a random state for reproducibility\n",
    "random_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space using distributions for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(100, 1000),  # Number of trees (100 to 1000)\n",
    "    \"max_depth\": randint(3, 10),  # Maximum depth of trees (3 to 9)\n",
    "    \"min_samples_split\": randint(2, 10),  # Minimum samples to split a node (2 to 9)\n",
    "    \"min_samples_leaf\": randint(1, 5)  # Minimum samples allowed in a leaf node (1 to 4)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for RandomForestRegressor\n",
    "random_search_rf = RandomizedSearchCV(estimator=random_forest, param_distributions=param_dist, n_iter=100, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search_rf.fit(X_traini, y_traini)\n",
    "\n",
    "# Print the best parameters found for RandomForest\n",
    "print(\"Best parameters for RandomForestRegressor found:\")\n",
    "print(random_search_rf.best_params_)\n",
    "\n",
    "# Get the best model from RandomizedSearchCV\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "def evaluate_model(model, X_testi, y_testi):\n",
    "    y_pred = model.predict(X_testi)\n",
    "    mse = mean_squared_error(y_testi, y_pred)\n",
    "    r2 = r2_score(y_testi, y_pred)\n",
    "    mae = mean_absolute_error(y_testi, y_pred)\n",
    "    return mse, r2, mae\n",
    "\n",
    "# Evaluate RandomForestRegressor\n",
    "mse_rf, r2_rf, mae_rf = evaluate_model(best_rf_model, X_testi, y_testi)\n",
    "print(f\"RandomForestRegressor Test Set Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_rf:.4f}\")\n",
    "print(f\"R-squared (R²) Score: {r2_rf:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Initialize the RandomForestRegressor with a random state for reproducibility\n",
    "random_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_dist = {\n",
    "    \"n_estimators\": range(100, 1000, 100),  # Number of trees (100 to 1000 with steps of 100)\n",
    "    \"max_depth\": range(3, 10),  # Maximum depth of trees (3 to 9)\n",
    "    \"min_samples_split\": range(2, 10),  # Minimum samples to split a node (2 to 9)\n",
    "    \"min_samples_leaf\": range(1, 5)  # Minimum samples allowed in a leaf node (1 to 4)\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for RandomForestRegressor\n",
    "grid_search_rf = GridSearchCV(estimator=random_forest, param_grid=param_dist, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_traini, y_traini)\n",
    "\n",
    "# Print the best parameters found for RandomForest\n",
    "print(\"Best parameters for RandomForestRegressor found:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "def evaluate_model(model, X_testi, y_testi):\n",
    "    y_pred = model.predict(X_testi)\n",
    "    mse = mean_squared_error(y_testi, y_pred)\n",
    "    r2 = r2_score(y_testi, y_pred)\n",
    "    mae = mean_absolute_error(y_testi, y_pred)\n",
    "    return mse, r2, mae\n",
    "\n",
    "# Evaluate RandomForestRegressor\n",
    "mse_rf, r2_rf, mae_rf = evaluate_model(best_rf_model, X_testi, y_testi)\n",
    "print(f\"RandomForestRegressor Test Set Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_rf:.4f}\")\n",
    "print(f\"R-squared (R²) Score: {r2_rf:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
    "print(\"=\"*50)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051fe49",
   "metadata": {},
   "source": [
    "While the Random Forest regressor shows promise with an average R² of 0.4586, the high standard deviations indicate that the \n",
    "model's performance is not stable. Addressing the variability and refining the model could lead to improved results. However, \n",
    "the computational requirements for such improvements are too demanding for my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bf2d5-8d22-4d25-8c65-bed9606f388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Split the dataset into features (X) and the target variable (y)\n",
    "Xii = merged_df[['Country Code', 'Longitude', 'Latitude', 'Average Cost for two', 'Aggregate rating', 'Votes']]  # Features (excluding categorical columns)\n",
    "X_categoricalii = pd.get_dummies(merged_df[['Has Table booking', 'Rating color', 'Rating text', 'Cuisines', 'Currency', 'Locality', 'Locality Verbose']])  # One-hot encode categorical columns\n",
    "Xii = pd.concat([Xii, X_categoricalii], axis=1)  # Combine numerical and encoded categorical features\n",
    "yii = merged_df['Price range']\n",
    "\n",
    "# Standardize features \n",
    "scaler = StandardScaler()\n",
    "X_scaledii = scaler.fit_transform(Xii)  # Fit and transform the scaled features\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_trainii, X_testii, y_trainii, y_testii = train_test_split(X_scaledii, yii, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73def7d9-5174-43a4-910e-ae5646ab4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define regression models\n",
    "regressors = {\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Evaluate and print results for each regressor\n",
    "for name, reg in regressors.items():\n",
    "    # Perform 5-fold cross-validation for MSE\n",
    "    scores_mse = cross_val_score(reg, Xii, yii, cv=5, scoring='neg_mean_squared_error')\n",
    "    # Perform 5-fold cross-validation for R²\n",
    "    scores_r2 = cross_val_score(reg, Xii, yii, cv=5, scoring='r2')\n",
    "    \n",
    "    # Compute mean and standard deviation of each metric\n",
    "    mean_mse, std_mse = -scores_mse.mean(), scores_mse.std()\n",
    "    mean_r2, std_r2 = scores_r2.mean(), scores_r2.std()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Regressor: {name}\")\n",
    "    print(f\"Mean Squared Error: {mean_mse:.4f} +/- {std_mse:.4f}\")\n",
    "    print(f\"R-squared (R²) Score: {mean_r2:.4f} +/- {std_r2:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bd6c185",
   "metadata": {},
   "source": [
    "Given these insights, while both GradientBoosting and RandomForest are strong contenders, GradientBoosting slightly edges out as\n",
    "the top performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hypertuning')\n",
    "\n",
    "''' from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize the GradientBoostingRegressor with a random state for reproducibility\n",
    "gradient_boosting = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_dist = {\n",
    "    \"n_estimators\": range(100, 1000, 100),  # Number of boosting stages (100 to 1000 with steps of 100)\n",
    "    \"learning_rate\": [0.1, 0.01, 0.05],  # Learning rates to try\n",
    "    \"max_depth\": range(3, 8),  # Maximum depth of trees (3 to 7)\n",
    "    \"min_samples_split\": range(2, 10),  # Minimum samples to split a node (2 to 9)\n",
    "    \"min_samples_leaf\": range(1, 5),  # Minimum samples in a leaf node (1 to 4)\n",
    "    \"loss\": [\"squared_error\", \"huber\"]  # Different loss functions\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for GradientBoostingRegressor\n",
    "grid_search_gb = GridSearchCV(estimator=gradient_boosting, param_grid=param_dist, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search_gb.fit(X_trainii, y_trainii)\n",
    "\n",
    "# Print the best parameters found for GradientBoostingRegressor\n",
    "print(\"Best parameters for GradientBoostingRegressor found:\")\n",
    "print(grid_search_gb.best_params_)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "def evaluate_model(model, X_testii, y_testii):\n",
    "    y_pred = model.predict(X_testii)\n",
    "    mse = mean_squared_error(y_testii, y_pred)\n",
    "    r2 = r2_score(y_testii, y_pred)\n",
    "    mae = mean_absolute_error(y_testii, y_pred)\n",
    "    return mse, r2, mae\n",
    "\n",
    "# Evaluate GradientBoostingRegressor\n",
    "mse_gb, r2_gb, mae_gb = evaluate_model(best_gb_model, X_testii, y_testii)\n",
    "print(f\"GradientBoostingRegressor Test Set Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_gb:.4f}\")\n",
    "print(f\"R-squared (R²) Score: {r2_gb:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_gb:.4f}\")\n",
    "print(\"=\"*50)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffe172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define regression models\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation for MSE\n",
    "scores_mse = cross_val_score(regressor, Xii, yii, cv=5, scoring='neg_mean_squared_error')\n",
    "# Perform 5-fold cross-validation for R²\n",
    "scores_r2 = cross_val_score(regressor, Xii, yii, cv=5, scoring='r2')\n",
    "\n",
    "# Compute mean and standard deviation of each metric\n",
    "mean_mse, std_mse = -scores_mse.mean(), scores_mse.std()\n",
    "mean_r2, std_r2 = scores_r2.mean(), scores_r2.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Regressor: Gradient Boosting\")\n",
    "print(f\"Mean Squared Error: {mean_mse:.4f} +/- {std_mse:.4f}\")\n",
    "print(f\"R-squared (R²) Score: {mean_r2:.4f} +/- {std_r2:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train the Random Forest regressor on the full training set\n",
    "random_forest = RandomForestRegressor()\n",
    "random_forest.fit(X_traini, y_traini)\n",
    "\n",
    "# Save the trained Random Forest model using joblib\n",
    "joblib_filename = \"RandomForest_modelZomato.joblib\"\n",
    "joblib.dump(random_forest, joblib_filename)\n",
    "print(f\"Saved Random Forest model to {joblib_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbfff3",
   "metadata": {},
   "source": [
    "The Gradient Boosting regressor shows strong performance with an average R² of 0.7280 and a low MSE of 0.0066. However, the\n",
    "high standard deviations for both metrics indicate significant variability in model performance across different data subsets.\n",
    "Improving data preprocessing, hyperparameter tuning, could enhance model stability and performance. Given the computational\n",
    "demands simpler models might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764cc93",
   "metadata": {},
   "source": [
    "Zomato Data Analysis is invaluable for foodies seeking the best cuisines worldwide that fit within their budget. The analysis \n",
    "helps individuals find value-for-money restaurants in different regions for various cuisines. This analysis also aids those \n",
    "looking to experience the finest local cuisines of a country and identifies the localities with the highest concentration of \n",
    "restaurants serving those cuisines. While the Random Forest regressor showed promise, the high variability in its performance \n",
    "indicated instability. However, the Gradient Boosting regressor demonstrated strong and more consistent performance, making it\n",
    "a better choice for this analysis despite potential computational demands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
